{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c75537cb",
   "metadata": {},
   "source": [
    "# Data Science Society Kmeans clustering (solutions)\n",
    "\n",
    "## Debugging problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ea9321",
   "metadata": {},
   "source": [
    "### 1) Read in the data and transform it for clustering\n",
    "\n",
    "Read in the whole dataset as we did before but now focusing on the five columns of: \n",
    "- residential_pop\n",
    "- class_1_pop\n",
    "- WZ_area_pop\n",
    "- transport_nearby\n",
    "- stores_nearby\n",
    "\n",
    "Transform and standardise the data ready for clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae53be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import contextily as cx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b59ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the stores dataset\n",
    "#it is a geographic dataset so we need to use\n",
    "#geopandas\n",
    "London_convenience_stores = gpd.read_file(??,\n",
    "                              driver = \"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c9afe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the columns we are interested in\n",
    "columns = [??]\n",
    "#set the histogram titles\n",
    "hist_titles = [\"Residential population nearby distribution\",\n",
    "              \"Social Class 1 nearby distribution\",\n",
    "              \"Daytime population nearby distribution\",\n",
    "              \"Transport stops nearby distribution\",\n",
    "              \"Retail stores nearby distribution\"]\n",
    "#set the acis labels\n",
    "axis_labels = [\"Population\", \n",
    "              \"Percentage of population\",\n",
    "              \"Population\",\n",
    "              \"Number of transport stops\",\n",
    "              \"Number of stores\"]\n",
    "\n",
    "#create a base axis for teh plots\n",
    "fig, ax = plt.subplots(3,2, figsize = (20,20))\n",
    "#flatten the axis to make it easy to iteratre over\n",
    "axis = ax.flatten()\n",
    "\n",
    "#iterate over each columns using the labels information already set up\n",
    "for i, col in enumerate(columns):\n",
    "    \n",
    "    #create the histogram using the column\n",
    "    ??[col].hist(bins = 100, ax = axis[i],\n",
    "                                       color = \"red\",\n",
    "                                       alpha = 0.7)\n",
    "    #add label information\n",
    "    axis[i].set_title(hist_titles[i], fontsize = 25, pad = 25)\n",
    "    axis[i].set_ylabel(\"Frequency\", fontsize  =20, labelpad = 30)\n",
    "    axis[i].set_xlabel(f\"{axis_labels[i]}\", fontsize = 20, labelpad = 20)\n",
    "    axis[i].tick_params(axis = \"both\", labelsize = 20)\n",
    "\n",
    "#remove the unused axis\n",
    "axis[5].set_axis_off()\n",
    "#keep the layout tight\n",
    "plt.tight_layout()\n",
    "#show the plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b253f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#understand the data distribution to see if \n",
    "#tehre will be any issues in trasnformation\n",
    "London_convenience_stores[[??]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9568b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iteratre over the columns\n",
    "#to turn them into log\n",
    "for col in columns:\n",
    "    #create a new log column and convert the data\n",
    "    London_convenience_stores[col + \"_log\"] = np.log10(London_convenience_stores[col] + 1)\n",
    "    #some issues are encountered so convert these to 0 values \n",
    "    London_convenience_stores[col + \"_log\"] = London_convenience_stores[col + \"_log\"].replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3505e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#copy the target columns and a dataframe to create\n",
    "#a new dataset\n",
    "scaled = ??\n",
    "\n",
    "#use the standard scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "#implement the scaling\n",
    "for col in scaled.columns:\n",
    "    scaled[col] = sc.fit_transform(??)\n",
    "\n",
    "#create a dataframe from the data\n",
    "log_then_normal = pd.DataFrame(scaled)\n",
    "\n",
    "#set the columns \n",
    "log_columns = [\"residential_pop_log\", \n",
    "               \"class_1_pop_log\", \n",
    "               \"WZ_area_pop_log\", \n",
    "               \"transport_nearby_log\", \n",
    "               \"stores_nearby_log\"]\n",
    "\n",
    "#set the column names\n",
    "log_then_normal.columns = log_columns\n",
    "\n",
    "#add this to a scaled dataset to be used in modelling\n",
    "London_convenience_stores_scaled = pd.concat([London_convenience_stores.drop(log_columns, axis = 1), \n",
    "                                              log_then_normal], \n",
    "                                             axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e82b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repaet the distribution plots to see how this has\n",
    "#changed\n",
    "fig, ax = plt.subplots(3,2, figsize = (20,20))\n",
    "#flatten the axis to make it easy to iteratre over\n",
    "axis = ax.flatten()\n",
    "\n",
    "#iterate over each columns using the labels information already set up\n",
    "for i, col in enumerate(columns):\n",
    "    \n",
    "    #create the histogram using the column\n",
    "    London_convenience_stores_scaled[??].hist(bins = 100, ax = axis[i],\n",
    "                                       color = \"red\",\n",
    "                                       alpha = 0.7)\n",
    "    #add label information\n",
    "    axis[i].set_title(hist_titles[i], fontsize = 25, pad = 25)\n",
    "    axis[i].set_ylabel(\"Frequency\", fontsize  =20, labelpad = 30)\n",
    "    axis[i].set_xlabel(f\"{axis_labels[i]}\", fontsize = 20, labelpad = 20)\n",
    "    axis[i].tick_params(axis = \"both\", labelsize = 20)\n",
    "\n",
    "#remove the unused axis\n",
    "??\n",
    "#keep the layout tight\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bc4a7f",
   "metadata": {},
   "source": [
    "### 2) Perform Silhouette and Elbow Analysis\n",
    "\n",
    "Using this transformed data, perform silhouette and elbow analysis on the data to find the optimal number of clusters to implement in the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d072b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "from sklearn import ??\n",
    "from sklearn.cluster import ??\n",
    "\n",
    "#create an empty list\n",
    "list_SSE = []\n",
    "#set ther ange of clusters to evaluate\n",
    "min_k = 1\n",
    "max_k = 10\n",
    "range_k = range(min_k, max_k)\n",
    "\n",
    "#iterate over the range\n",
    "for i in range_k:\n",
    "    #perform the clustering algorithm\n",
    "    km = KMeans(n_clusters = ??,\n",
    "               init = \"random\",\n",
    "               n_init = ??,\n",
    "               max_iter = ??,\n",
    "               tol = 1e-04, \n",
    "                random_state = 22)\n",
    "    #fit this to the data\n",
    "    km.fit(??[log_columns])\n",
    "    #add the SEE score\n",
    "    list_SSE.append(km.??)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb6a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the silhouette score\n",
    "import random\n",
    "\n",
    "#create an empty list\n",
    "#called silhouette\n",
    "??\n",
    "\n",
    "#iteratre over the number of clusters to evaluate\n",
    "for i in range(2,10):\n",
    "    \n",
    "    #create an empty list to hold the averages\n",
    "    average = []\n",
    "    #perform the clustering algorithm several times for each number of clusters\n",
    "    for x in range(1,10):\n",
    "        #set the number of clusters\n",
    "        k_cluster = ??\n",
    "        #generate a arandom seed number\n",
    "        random_seed = random.randint(1,101)\n",
    "        #apply the KMeans clustering algorithm\n",
    "        kmeans_method = KMeans(n_clusters = ??,\n",
    "                              random_state = ??)\n",
    "        kmeans_method.fit(??)\n",
    "        #extract the labels\n",
    "        labels = kmeans_method.??\n",
    "        #extract the silhouette score\n",
    "        a = metrics.??(London_convenience_stores_scaled[log_columns], labels)\n",
    "        #append the result\n",
    "        average.append(a)\n",
    "        \n",
    "    #clauclate the average silhouette score for each number of clusters \n",
    "    silhouette.append(sum(average)/len(average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ba0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine both plots\n",
    "fig, ax = plt.subplots(1,2, figsize = (15,8))\n",
    "\n",
    "ax[0].plot(range(2,10), ??, marker = \"o\")\n",
    "ax[0].set_xlabel(\"Number of Clusters\", fontsize = 20, labelpad = 20)\n",
    "ax[0].set_ylabel(\"Silhoute score\", fontsize =20, labelpad = 20)\n",
    "ax[0].set_title(\"Kmeans silhouette plot\", fontsize = 25, pad = 20)\n",
    "\n",
    "ax[1].plot(range_k, ??, marker = \"o\")\n",
    "ax[1].set_xlabel(\"Number of Clusters\", fontsize = 20, labelpad = 20)\n",
    "ax[1].set_ylabel(\"SSE\", fontsize =20, labelpad = 20)\n",
    "ax[1].set_title(\"Kmeans Elbow plot\", fontsize = 25, pad = 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d3462",
   "metadata": {},
   "source": [
    "### 3) Select the most appropiate cluster number and check the results\n",
    "\n",
    "Using the plots created above, which is the most appropriate number of clusters to implement on the dataset? Apply the model and check the results both in terms of the means of each clusters and the geographical interpretation. Create cluster names and descriptions for these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40183ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform the clustering with the chosen number of clusters\n",
    "kmeans_method = KMeans(n_clusters = ??,\n",
    "                        random_state = 22,\n",
    "                      n_init = 10)\n",
    "\n",
    "kmeans_method.fit(??)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17851855",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#set the titles\n",
    "titles = [\"Residential population per cluster\", \n",
    "         \"Percentage of class 1 population per cluster\",\n",
    "         \"Daytime population per cluster\",\n",
    "         \"Tranport Stops per cluster\",\n",
    "         \"Retail outlets per cluster\"]\n",
    "#set the labels\n",
    "ylabels = [\"Population\",\n",
    "          \"Percentage (%)\",\n",
    "          \"Population\",\n",
    "          \"Transport Stops\",\n",
    "          \"Retail outlets\"]\n",
    "#extract the cluster labels and add these back to teh data\n",
    "labels = ??\n",
    "London_convenience_stores[\"labels\"] = ??\n",
    "London_convenience_stores[\"labels\"] = London_convenience_stores[\"labels\"] + 1\n",
    "\n",
    "#create a base plot\n",
    "fig, ax = plt.subplots(3,2, figsize = (20,20))\n",
    "#flatten the axis\n",
    "axis = ax.flatten()\n",
    "\n",
    "#iterate over each column to create plots for each \n",
    "for i, col in enumerate(??):\n",
    "    #create an empty dictionary\n",
    "    col_dict = {}\n",
    "    #iterate over each label\n",
    "    for label in list(??):\n",
    "        #crete a new dataframe for each label\n",
    "        label_df = London_convenience_stores[London_convenience_stores[\"labels\"] == label]\n",
    "        #add the mean to the dataframe\n",
    "        col_dict[label] = label_df[col].mean()\n",
    "    #convert the dictionary to a dataframe\n",
    "    column_df = pd.DataFrame.from_dict(col_dict, orient = \"index\")\n",
    "    #reset the index\n",
    "    column_df.reset_index(inplace=True)\n",
    "    #sort the values by the index\n",
    "    column_df.sort_values(by = \"index\", inplace=True)\n",
    "    \n",
    "    #plot the results\n",
    "    axis[i].plot(column_df[\"index\"], column_df[0],\n",
    "                marker = \"o\")\n",
    "    \n",
    "    #set the plots up\n",
    "    axis[i].set_title(titles[i], fontsize = 25, pad = 25)\n",
    "    axis[i].set_xlabel(\"Cluster\", fontsize = 25, labelpad = 25)\n",
    "    axis[i].set_ylabel(ylabels[i], fontsize =25, labelpad = 25)\n",
    "    axis[i].tick_params(axis = \"both\", labelsize = 20)\n",
    "\n",
    "#remove the unused axis\n",
    "axis[5].set_axis_off()\n",
    "\n",
    "#set the layout to tight so no overalp\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2974ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the number of stores within each \n",
    "London_convenience_stores[\"labels\"].??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee8729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in Output areas outline from the UK geodata store\n",
    "#read in the output area shapefile\n",
    "#extracted from the UK geo portal: https://geoportal.statistics.gov.uk/\n",
    "London_outline = gpd.read_file(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cf72e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the base axis\n",
    "fig, ax = plt.subplots(1,1, figsize = (12,12))\n",
    "\n",
    "#plot the boundary\n",
    "London_outline.boundary.plot(ax = ax,\n",
    "                        color = \"black\")\n",
    "\n",
    "#add the labels\n",
    "??.plot(column = ??,\n",
    "                   categorical = True,\n",
    "                   legend = True,\n",
    "                   ax = ax,\n",
    "                   cmap = \"tab10\",\n",
    "                   alpha = 0.7,\n",
    "                   legend_kwds = {\"title\":\"Cluster\",\n",
    "                                 \"fontsize\":\"20\",\n",
    "                                 \"title_fontsize\":\"25\"})\n",
    "\n",
    "#add the basemap\n",
    "cx.add_basemap(crs = \"EPSG:27700\",\n",
    "              ax = ax)\n",
    "\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5f5e1",
   "metadata": {},
   "source": [
    "Describe the clusters with both a name and a rough description:\n",
    "\n",
    "Do these make sense to you?\n",
    "\n",
    "What would you do differently?\n",
    "\n",
    "How can you get better results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6d638",
   "metadata": {},
   "source": [
    "## Coding Challenges\n",
    "\n",
    "### Apply the model to a dataset of you choosing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSS",
   "language": "python",
   "name": "dss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
